{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning",
      "provenance": [],
      "authorship_tag": "ABX9TyOhEEEZM1wiXVb1V/QSa8EY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taniokah/DL-Basic-Seminar2/blob/master/Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ZnPF0YT9QJ",
        "colab_type": "text"
      },
      "source": [
        "# Fine Tuning\n",
        "\n",
        "VGG16を用いて転移学習を実験します。\n",
        "対象データは、apple、strawberry、tomato の3種類の画像を用います。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q_ili_BUH9j",
        "colab_type": "code",
        "outputId": "89fa9131-3ad7-48cd-830a-b13a9663715a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!pip install pillow\n",
        "!pip install icrawler"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (6.2.2)\n",
            "Requirement already satisfied: icrawler in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from icrawler) (4.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from icrawler) (6.2.2)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from icrawler) (2.21.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from icrawler) (4.2.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from icrawler) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->icrawler) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->icrawler) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->icrawler) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->icrawler) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaiE_LuwWmdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from icrawler.builtin import BaiduImageCrawler, BingImageCrawler, GoogleImageCrawler\n",
        "\n",
        "bing_crawler = BingImageCrawler(storage={'root_dir': 'images/train/apple'}, downloader_threads=4)\n",
        "bing_crawler.crawl(keyword='apple', filters=None, offset=0, max_num=1000) \n",
        "bing_crawler = BingImageCrawler(storage={'root_dir': 'images/train/strawberry'}, downloader_threads=4)\n",
        "bing_crawler.crawl(keyword='strawberry', filters=None, offset=0, max_num=1000)\n",
        "bing_crawler = BingImageCrawler(storage={'root_dir': 'images/train/tomato'}, downloader_threads=4)\n",
        "bing_crawler.crawl(keyword='tomato', filters=None, offset=0, max_num=1000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM4eoHGIOJQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baidu_crawler = BaiduImageCrawler(storage={'root_dir': 'images/validation/apple'}, downloader_threads=4)\n",
        "baidu_crawler.crawl(keyword='apple', offset=0, max_num=200)\n",
        "baidu_crawler = BaiduImageCrawler(storage={'root_dir': 'images/validation/strawberry'}, downloader_threads=4)\n",
        "baidu_crawler.crawl(keyword='strawberry', offset=0, max_num=200)\n",
        "baidu_crawler = BaiduImageCrawler(storage={'root_dir': 'images/validation/tomato'}, downloader_threads=4)\n",
        "baidu_crawler.crawl(keyword='tomato', offset=0, max_num=200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "456LEvGdbl0i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9482948b-86b2-479b-94fc-2141768b0d87"
      },
      "source": [
        "# https://qiita.com/nirs_kd56/items/7a7330b160fd1604197e\n",
        "\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "N_CATEGORIES  = 3\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "NUM_TRAINING = 1000\n",
        "NUM_VALIDATION = 200\n",
        "\n",
        "input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "base_model = VGG16(weights='imagenet', \n",
        "                   include_top=False, \n",
        "                   input_tensor=input_tensor)\n",
        "\n",
        "# 全結合層の構築\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "top_model.add(Dense(256))\n",
        "top_model.add(Activation(\"sigmoid\"))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(128))\n",
        "top_model.add(Activation(\"sigmoid\"))\n",
        "top_model.add(Dense(N_CATEGORIES))\n",
        "top_model.add(Activation(\"softmax\"))\n",
        "\n",
        "model = Model(input=base_model.input, \n",
        "              output=top_model(base_model.output))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='sgd',  \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "   rescale=1.0 / 255,\n",
        "   shear_range=0.2,\n",
        "   zoom_range=0.2,\n",
        "   horizontal_flip=True,\n",
        "   rotation_range=10)\n",
        "\n",
        "test_datagen = image.ImageDataGenerator(\n",
        "   rescale=1.0 / 255,\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "   'images/train',\n",
        "   target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "   batch_size=BATCH_SIZE,\n",
        "   class_mode='categorical',\n",
        "   shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "   'images/validation',\n",
        "   target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "   batch_size=BATCH_SIZE,\n",
        "   class_mode='categorical',\n",
        "   shuffle=True\n",
        ")\n",
        "\n",
        "hist = model.fit_generator(train_generator,\n",
        "   steps_per_epoch=NUM_TRAINING//BATCH_SIZE,\n",
        "   epochs=30,\n",
        "   verbose=1,\n",
        "   validation_data=validation_generator,\n",
        "   validation_steps=NUM_VALIDATION//BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# 学習結果を表示\n",
        "loss = hist.history['loss']\n",
        "acc = hist.history['acc']\n",
        "val_loss = hist.history['val_loss']\n",
        "val_acc = hist.history['val_acc']\n",
        "epochs = len(loss)\n",
        "plt.plot(range(epochs), loss, marker='.', label='loss(training data)')\n",
        "plt.plot(range(epochs), acc, marker='.', label='acc(training data)')\n",
        "plt.plot(range(epochs), val_loss, marker='.', label='val_loss(evaluationdata)')\n",
        "plt.plot(range(epochs), val_acc, marker='.', label='val_acc(evaluationdata)')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss/acc')\n",
        "plt.show()\n",
        "\n",
        "# モデルを保存\n",
        "model.save('fruits2.hdf5')\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open('model2.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save_weights(\"model2.h5\")\n",
        "print(\"saved model..! ready to go.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 3)                 6456067   \n",
            "=================================================================\n",
            "Total params: 21,170,755\n",
            "Trainable params: 6,456,067\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Found 1669 images belonging to 3 classes.\n",
            "Found 491 images belonging to 3 classes.\n",
            "Epoch 1/30\n",
            "62/62 [==============================] - 58s 929ms/step - loss: 1.0922 - acc: 0.3790 - val_loss: 1.0948 - val_acc: 0.2969\n",
            "Epoch 2/30\n",
            "61/62 [============================>.] - ETA: 0s - loss: 1.0742 - acc: 0.3928"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:784: UserWarning: Possibly corrupt EXIF data.  Expecting to read 33 bytes but only got 19. Skipping tag 42016\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 55s 895ms/step - loss: 1.0742 - acc: 0.3895 - val_loss: 1.0751 - val_acc: 0.4427\n",
            "Epoch 3/30\n",
            " 1/62 [..............................] - ETA: 9s - loss: 1.1312 - acc: 0.2500"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 47s 753ms/step - loss: 1.0303 - acc: 0.4540 - val_loss: 1.0238 - val_acc: 0.5615\n",
            "Epoch 4/30\n",
            "62/62 [==============================] - 45s 730ms/step - loss: 1.0041 - acc: 0.5111 - val_loss: 0.9765 - val_acc: 0.5885\n",
            "Epoch 5/30\n",
            "62/62 [==============================] - 46s 735ms/step - loss: 0.9544 - acc: 0.5468 - val_loss: 1.0070 - val_acc: 0.5156\n",
            "Epoch 6/30\n",
            "62/62 [==============================] - 48s 769ms/step - loss: 0.9479 - acc: 0.5671 - val_loss: 0.9643 - val_acc: 0.5080\n",
            "Epoch 7/30\n",
            "62/62 [==============================] - 47s 750ms/step - loss: 0.9017 - acc: 0.6069 - val_loss: 0.9293 - val_acc: 0.6146\n",
            "Epoch 8/30\n",
            "62/62 [==============================] - 45s 732ms/step - loss: 0.8949 - acc: 0.6054 - val_loss: 0.9226 - val_acc: 0.6150\n",
            "Epoch 9/30\n",
            "62/62 [==============================] - 45s 728ms/step - loss: 0.8806 - acc: 0.6129 - val_loss: 0.8750 - val_acc: 0.5833\n",
            "Epoch 10/30\n",
            "62/62 [==============================] - 52s 842ms/step - loss: 0.8486 - acc: 0.6276 - val_loss: 0.8749 - val_acc: 0.6406\n",
            "Epoch 11/30\n",
            "62/62 [==============================] - 50s 804ms/step - loss: 0.8250 - acc: 0.6669 - val_loss: 0.9214 - val_acc: 0.5668\n",
            "Epoch 12/30\n",
            "62/62 [==============================] - 51s 820ms/step - loss: 0.7895 - acc: 0.6845 - val_loss: 0.8474 - val_acc: 0.6302\n",
            "Epoch 13/30\n",
            "62/62 [==============================] - 49s 798ms/step - loss: 0.7807 - acc: 0.6913 - val_loss: 0.8497 - val_acc: 0.6150\n",
            "Epoch 14/30\n",
            "62/62 [==============================] - 46s 742ms/step - loss: 0.7446 - acc: 0.7127 - val_loss: 0.8199 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "62/62 [==============================] - 48s 769ms/step - loss: 0.7388 - acc: 0.7175 - val_loss: 0.8076 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "62/62 [==============================] - 47s 752ms/step - loss: 0.7350 - acc: 0.6976 - val_loss: 0.8134 - val_acc: 0.6898\n",
            "Epoch 17/30\n",
            "62/62 [==============================] - 52s 838ms/step - loss: 0.7242 - acc: 0.7137 - val_loss: 0.7868 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "62/62 [==============================] - 54s 868ms/step - loss: 0.6968 - acc: 0.7288 - val_loss: 0.7749 - val_acc: 0.6771\n",
            "Epoch 19/30\n",
            "62/62 [==============================] - 55s 883ms/step - loss: 0.6877 - acc: 0.7206 - val_loss: 0.8115 - val_acc: 0.6364\n",
            "Epoch 20/30\n",
            "62/62 [==============================] - 53s 850ms/step - loss: 0.6525 - acc: 0.7486 - val_loss: 0.7878 - val_acc: 0.6771\n",
            "Epoch 21/30\n",
            "12/62 [====>.........................] - ETA: 46s - loss: 0.6836 - acc: 0.7292"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzi4pId3sB8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/parthvadhadiya/classify_dogs-vs-cats_using_keras/blob/master/use_model.py\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.preprocessing import image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def loadmodel(m1='model.json', m2='model.h5'):\n",
        "    json_file = open(m1, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "    loaded_model.load_weights(m2)\n",
        "    print(\"Loaded model from disk\")\n",
        "\n",
        "    '''loaded_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adadelta(),\n",
        "                  metrics=['accuracy'])\n",
        "    '''\n",
        "    loaded_model.compile(optimizer = 'SGD', \n",
        "                         loss = 'categorical_crossentropy', \n",
        "                         metrics = ['accuracy'])\n",
        "    return loaded_model\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = IMAGE_SIZE, IMAGE_SIZE\n",
        "\n",
        "# 画像表示のための関数\n",
        "def showimg(filename, title, i):\n",
        "    im = ImageFile.open(filename)\n",
        "    im_list = np.asarray(im)\n",
        "    plt.subplot(2, 5, i)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(im_list)\n",
        "\n",
        "# 画像判定のための関数\n",
        "def predictimg(filename, featuresize):\n",
        "    img = ImageFile.load_img(filename, target_size = (img_width, img_height))\n",
        "    x = ImageFile.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis = 0)\n",
        "    #preds = model.predict(preprocess_input(x))\n",
        "    preds = model.predict(x)\n",
        "    #results = decode_predictions(preds, top = featuresize)[0]\n",
        "    #return results\n",
        "    return preds\n",
        "\n",
        "# 画像を判定\n",
        "def predict(filename, label):\n",
        "    results = predictimg(filename, 10)\n",
        "    #print(results)\n",
        "    index = np.argmax(results[0])\n",
        "    if index == 0:\n",
        "        label += ' = ' + 'apple'\n",
        "    elif index == 1:\n",
        "        label += ' = ' + 'strawberry'\n",
        "    else:\n",
        "        label += ' = ' + 'tomato'\n",
        "    plt.figure(figsize = (20, 10))\n",
        "    showimg(filename, label, 1)\n",
        "    plt.show()\n",
        "\n",
        "model = loadmodel('model2.json', 'model2.h5')\n",
        "\n",
        "predict('images/validation/apple/000001.jpg', 'apple')\n",
        "predict('images/validation/apple/000002.jpg', 'apple')\n",
        "predict('images/validation/apple/000003.jpg', 'apple')\n",
        "predict('images/validation/apple/000004.jpg', 'apple')\n",
        "predict('images/validation/apple/000005.jpg', 'apple')\n",
        "\n",
        "predict('images/validation/strawberry/000001.jpeg', 'strawberry')\n",
        "predict('images/validation/strawberry/000002.jpg', 'strawberry')\n",
        "predict('images/validation/strawberry/000003.jpeg', 'strawberry')\n",
        "predict('images/validation/strawberry/000004.jpg', 'strawberry')\n",
        "predict('images/validation/strawberry/000005.jpg', 'strawberry')\n",
        "\n",
        "predict('images/validation/tomato/000001.jpg', 'tomato')\n",
        "predict('images/validation/tomato/000002.jpg', 'tomato')\n",
        "predict('images/validation/tomato/000003.jpg', 'tomato')\n",
        "predict('images/validation/tomato/000004.jpg', 'tomato')\n",
        "predict('images/validation/tomato/000005.png', 'tomato')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mAsTjcQZ1TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://spjai.com/keras-fine-tuning/\n",
        "\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "N_CATEGORIES  = 3\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "NUM_TRAINING = 1000\n",
        "NUM_VALIDATION = 200\n",
        "\n",
        "input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "base_model = VGG16(weights='imagenet', \n",
        "                   include_top=False, \n",
        "                   input_tensor=input_tensor)\n",
        "\n",
        "# 全結合層の構築\n",
        "top_model = Sequential()\n",
        "top_model.add(GlobalAveragePooling2D())\n",
        "top_model.add(Dense(1024, activation='relu'))\n",
        "top_model.add(Dense(N_CATEGORIES, activation='softmax'))\n",
        "\n",
        "model = Model(inputs=base_model.input, \n",
        "              outputs=top_model(base_model.output))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=SGD(lr=0.0001, momentum=0.9), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "train_datagen = ImageFile.ImageDataGenerator(\n",
        "   rescale=1.0 / 255,\n",
        "   shear_range=0.2,\n",
        "   zoom_range=0.2,\n",
        "   horizontal_flip=True,\n",
        "   rotation_range=10)\n",
        "\n",
        "test_datagen = ImageFile.ImageDataGenerator(\n",
        "   rescale=1.0 / 255,\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "   'images/train',\n",
        "   target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "   batch_size=BATCH_SIZE,\n",
        "   class_mode='categorical',\n",
        "   shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "   'images/validation',\n",
        "   target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "   batch_size=BATCH_SIZE,\n",
        "   class_mode='categorical',\n",
        "   shuffle=True\n",
        ")\n",
        "\n",
        "hist = model.fit_generator(train_generator,\n",
        "   steps_per_epoch=NUM_TRAINING//BATCH_SIZE,\n",
        "   epochs=30,\n",
        "   verbose=1,\n",
        "   validation_data=validation_generator,\n",
        "   validation_steps=NUM_VALIDATION//BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# 学習結果を表示\n",
        "loss = hist.history['loss']\n",
        "acc = hist.history['acc']\n",
        "val_loss = hist.history['val_loss']\n",
        "val_acc = hist.history['val_acc']\n",
        "epochs = len(loss)\n",
        "plt.plot(range(epochs), loss, marker='.', label='loss(training data)')\n",
        "plt.plot(range(epochs), acc, marker='.', label='acc(training data)')\n",
        "plt.plot(range(epochs), val_loss, marker='.', label='val_loss(evaluationdata)')\n",
        "plt.plot(range(epochs), val_acc, marker='.', label='val_acc(evaluationdata)')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss/acc')\n",
        "plt.show()\n",
        "\n",
        "# モデルを保存\n",
        "model.save('fruits.hdf5')\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"saved model..! ready to go.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n2S0jIbft74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadmodel(m1='model.json', m2='model.h5'):\n",
        "    json_file = open(m1, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "    loaded_model.load_weights(m2)\n",
        "    print(\"Loaded model from disk\")\n",
        "\n",
        "    '''loaded_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adadelta(),\n",
        "                  metrics=['accuracy'])\n",
        "    '''\n",
        "    loaded_model.compile(optimizer = 'SGD', \n",
        "                         loss = 'categorical_crossentropy', \n",
        "                         metrics = ['accuracy'])\n",
        "    return loaded_model\n",
        "\n",
        "model = loadmodel('model.json', 'model.h5')\n",
        "\n",
        "predict('images/validation/apple/000001.jpg', 'apple')\n",
        "predict('images/validation/apple/000002.jpg', 'apple')\n",
        "predict('images/validation/apple/000003.jpg', 'apple')\n",
        "predict('images/validation/apple/000004.jpg', 'apple')\n",
        "predict('images/validation/apple/000005.jpg', 'apple')\n",
        "\n",
        "predict('images/validation/strawberry/000001.jpeg', 'strawberry')\n",
        "predict('images/validation/strawberry/000002.jpg', 'strawberry')\n",
        "predict('images/validation/strawberry/000003.jpeg', 'strawberry')\n",
        "predict('images/validation/strawberry/000004.jpg', 'strawberry')\n",
        "predict('images/validation/strawberry/000005.jpg', 'strawberry')\n",
        "\n",
        "predict('images/validation/tomato/000001.jpg', 'tomato')\n",
        "predict('images/validation/tomato/000002.jpg', 'tomato')\n",
        "predict('images/validation/tomato/000003.jpg', 'tomato')\n",
        "predict('images/validation/tomato/000004.jpg', 'tomato')\n",
        "predict('images/validation/tomato/000005.png', 'tomato')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIPGLOhnvG4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}